{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to 20240710_180717.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Chrome 드라이버 경로 설정\n",
    "driver_path = 'C:/chromedriver-win64/chromedriver.exe'\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# 웹사이트 열기\n",
    "url = 'https://search.naver.com/search.naver?where=news&query=%EB%B9%84%ED%8A%B8%EC%BD%94%EC%9D%B8&sm=tab_opt&sort=1&photo=0&field=0&pd=4&ds=&de=&docid=&related=0&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so%3Add%2Cp%3A1d&is_sug_officeid=0&office_category=0&service_area=0'  # 대상 웹사이트 URL로 변경\n",
    "driver.get(url)\n",
    "\n",
    "# 페이지 배율 줄이기 (줌 아웃)\n",
    "driver.execute_script(\"document.body.style.zoom='50%'\")\n",
    "time.sleep(2)  # 페이지 로드 시간을 위해 잠시 대기\n",
    "\n",
    "# 페이지 스크롤 끝까지 내리기\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "\n",
    "# 데이터 수집\n",
    "news_data = []\n",
    "news_elements = driver.find_elements(By.CSS_SELECTOR, 'li.bx')\n",
    "\n",
    "for element in news_elements:\n",
    "    try:\n",
    "        link_element = element.find_element(By.CSS_SELECTOR, 'a[href*=\"https://n.news.naver.com/\"]')\n",
    "        link = link_element.get_attribute('href')\n",
    "        \n",
    "        # 제목 수집\n",
    "        title_element = element.find_element(By.CSS_SELECTOR, 'a.news_tit')\n",
    "        title = title_element.get_attribute('title')\n",
    "        \n",
    "        # 시간 수집 - span.info 요소에서 시간 텍스트 찾기\n",
    "        time_elements = element.find_elements(By.CSS_SELECTOR, 'span.info')\n",
    "        time_text = \"\"\n",
    "        for time_element in time_elements:\n",
    "            time_text = time_element.text\n",
    "            if \"분 전\" in time_text or \"시간 전\" in time_text or \"일 전\" in time_text:\n",
    "                break\n",
    "\n",
    "        news_data.append({\n",
    "            'title': title,\n",
    "            'time': time_text,\n",
    "            'link': link\n",
    "        })\n",
    "    except Exception as e:\n",
    "        # 예외 발생 시 무시하고 다음 요소로 넘어감\n",
    "        continue\n",
    "\n",
    "# 크롬 드라이버 종료\n",
    "driver.quit()\n",
    "\n",
    "# 수집한 데이터를 CSV 파일로 저장\n",
    "current_time = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "file_name = f'{current_time}.csv'\n",
    "\n",
    "with open(file_name, mode='w', newline='', encoding='utf-8-sig') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=['title', 'time', 'link'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(news_data)\n",
    "\n",
    "print(f'Data saved to {file_name}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 링크 수집\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 되는 것\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to 20240710_214046.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException, StaleElementReferenceException, ElementClickInterceptedException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Chrome 드라이버 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--start-maximized\")\n",
    "driver_path = 'C:/chromedriver-win64/chromedriver.exe'\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# 링크 목록을 저장한 CSV 파일 경로\n",
    "links_file = '0303.csv'  # 'links.csv' 파일은 현재 작업 디렉토리에 위치해야 합니다.\n",
    "\n",
    "# 링크 목록 읽기\n",
    "with open(links_file, mode='r', encoding='utf-8-sig') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    links = [{'title': row['title'], 'time': row['time'], 'link': row['link']} for row in reader]\n",
    "\n",
    "# 수집한 데이터를 저장할 리스트\n",
    "all_news_data = []\n",
    "\n",
    "for link_info in links:\n",
    "    driver.get(link_info['link'])\n",
    "    time.sleep(2)  # 페이지 로드 시간을 위해 잠시 대기\n",
    "\n",
    "    # 기사 제목 수집\n",
    "    title_element = driver.find_element(By.CSS_SELECTOR, 'h2#title_area span')\n",
    "    title = title_element.text\n",
    "\n",
    "    # 기사 입력 시간 수집\n",
    "    time_element = driver.find_element(By.CSS_SELECTOR, 'span.media_end_head_info_datestamp_time._ARTICLE_DATE_TIME')\n",
    "    article_time = time_element.get_attribute('data-date-time')\n",
    "\n",
    "    # 댓글 수 수집\n",
    "    comment_count_element = driver.find_element(By.CSS_SELECTOR, 'a#comment_count')\n",
    "    comment_count_text = comment_count_element.text\n",
    "    comment_count = 0 if 'is_zero' in comment_count_element.get_attribute('class') else int(comment_count_text)\n",
    "\n",
    "    if comment_count > 0:\n",
    "        # 댓글 페이지로 이동\n",
    "        comment_page_url = comment_count_element.get_attribute('href')\n",
    "        driver.get(comment_page_url)\n",
    "        \n",
    "        # 페이지 배율 줄이기 (줌 아웃)\n",
    "        driver.execute_script(\"document.body.style.zoom='50%'\")\n",
    "        time.sleep(2)  # 페이지 로드 시간을 위해 잠시 대기\n",
    "\n",
    "        # 클린봇 해제하여 모든 댓글을 볼 수 있도록 설정\n",
    "        try:\n",
    "            cleanbot = driver.find_element(By.CSS_SELECTOR, 'a.u_cbox_cleanbot_setbutton')\n",
    "            cleanbot.click()\n",
    "            time.sleep(1)\n",
    "            \n",
    "            cleanbot_disable = driver.find_element(By.XPATH, \"//input[@id='cleanbot_dialog_checkbox_cbox_module']\")\n",
    "            cleanbot_disable.click()\n",
    "            time.sleep(1)\n",
    "            \n",
    "            cleanbot_confirm = driver.find_element(By.CSS_SELECTOR, 'div.u_cbox_layer_cleanbot2_extra a')\n",
    "            cleanbot_confirm.click()\n",
    "            time.sleep(1)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # \"더보기\" 버튼 클릭을 통한 댓글 로딩\n",
    "        while True:\n",
    "            try:\n",
    "                more_button = WebDriverWait(driver, 5).until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, \"a.u_cbox_btn_more\"))\n",
    "                )\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView(true);\", more_button)\n",
    "                time.sleep(1)\n",
    "                more_button.click()\n",
    "                time.sleep(2)\n",
    "            except (NoSuchElementException, TimeoutException):\n",
    "                break\n",
    "            except ElementClickInterceptedException:\n",
    "                driver.execute_script(\"arguments[0].click();\", more_button)\n",
    "                time.sleep(2)\n",
    "\n",
    "        # 데이터 수집\n",
    "        while True:\n",
    "            try:\n",
    "                comments = driver.find_elements(By.CSS_SELECTOR, \".u_cbox_contents\")\n",
    "                comment_dates = driver.find_elements(By.CSS_SELECTOR, \".u_cbox_date\")\n",
    "                recommend_counts = driver.find_elements(By.CSS_SELECTOR, \".u_cbox_cnt_recomm\")\n",
    "                unrecommend_counts = driver.find_elements(By.CSS_SELECTOR, \".u_cbox_cnt_unrecomm\")\n",
    "                break\n",
    "            except StaleElementReferenceException:\n",
    "                time.sleep(0.5)\n",
    "\n",
    "        for i in range(len(comments)):\n",
    "            try:\n",
    "                comment = comments[i].text\n",
    "                comment_date = comment_dates[i].text\n",
    "                recommend_count = recommend_counts[i].text\n",
    "                unrecommend_count = unrecommend_counts[i].text\n",
    "\n",
    "                all_news_data.append({\n",
    "                    \"title\": title,\n",
    "                    \"time\": article_time,\n",
    "                    \"comment_count\": comment_count,\n",
    "                    \"comment\": comment,\n",
    "                    \"comment_date\": comment_date,\n",
    "                    \"recommend_count\": recommend_count,\n",
    "                    \"unrecommend_count\": unrecommend_count\n",
    "                })\n",
    "            except StaleElementReferenceException:\n",
    "                continue\n",
    "\n",
    "# 크롬 드라이버 종료\n",
    "driver.quit()\n",
    "\n",
    "# 수집한 데이터를 CSV 파일로 저장\n",
    "current_time = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "file_name = f'{current_time}.csv'  # 파일을 현재 작업 디렉토리에 저장\n",
    "\n",
    "with open(file_name, mode='w', newline='', encoding='utf-8-sig') as file:\n",
    "    fieldnames = ['title', 'time', 'comment_count', 'comment', 'comment_date', 'recommend_count', 'unrecommend_count']\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(all_news_data)\n",
    "\n",
    "print(f'Data saved to {file_name}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
